{
  "hash": "6a966b7dda8e682c6542f1b89a2254ae",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Numba \n---\n\n\n\n\n\n\n# What is Numba?\n\n- `Numba` addresses a long-standing problem in scientific computing.\n  \t- Writing code in Python, instead of C or Fortran, is fast.\n\t- Running code in Python, instead of C or Fortran, is slow.\n\n## C/Fortran\n\t  \n- C and Fortran have been swimming around the periphery for some time.\n  \t- E.g. NumPy is written in C, uses C numbers.\n\t- E.g. SymPy can print C/Fortran code for expressions.\n- C is a general purpose language and the highest performance language in existence.\n- Fortran, for \"formula transcription\", is the foremost numerical computing platform (for performance).\n\n## Scripting\n\n- Python is a *scripting* language.\n  \t- Write code in `.py` file or within `python`\n\t- The `python` program \"runs\" the code\n\t- No program is created.\n\n## Vs. C/Fortran\n\n- C and Fortran are *compiled* languages.\n  \t- Write code.\n\t- Call a program, a **compiler**, on the code.\n\t- The compiler creates a **program**.\n\t- Run the program.\n\n## Numba\n\n- Numba is a compiler for Python.\n- More than that - a jit (just-in-time) compiler.\n- Numba looks and feels like Python, but under the hood is compiling Python code in fast, compiled languages.\n- It is a good halfway step to avoid having to learn C/Fortran.\n\n## In its own words:\n\n\n> [Numba is a compiler for Python array and numerical functions that gives you the power to speed up your applications with high performance functions written directly in Python.](https://numba.readthedocs.io/en/stable/user/overview.html)\n\n> [With a few simple annotations, array-oriented and math-heavy Python code can be just-in-time optimized to performance similar as C, C++ and Fortran, without having to switch languages or Python interpreters.](https://numba.readthedocs.io/en/stable/user/overview.html)\n\n## Why not Numba?\n\n- Just learn C or Fortran\n- Use Cython, an alternative Python compiler.\n- Use SymPy print-to-C/Fortran\n- Use GPU acceleration via PyTorch or Tensorflow for certain applications.\n\t- The Meta and Google GPU-accelerated frameworks, respectively.\n\t- `numba-cuda` (NVIDIA GPU Numba) exists, but I haven't seen it used much.\n\n\n# Install\n\n## Pip again!\n\n- Nothing special here.\n```{.bash code-line-numbers=\"false\"}\npython3 -m pip install numba\n```\n- Let's try it out.\n\n## Decorators\n\n- We need to introduce something called a *decorator*\n- It's a little note before a function definition that starts with `@`\n  \t- This means we'll mostly write functions to use Numba.\n- We'll do an example.\n\n## Import\n\n- Numba is essential a NumPy optimizer, so we'll include both.\n\n::: {#cb6eebb3 .cell execution_count=2}\n``` {.python .cell-code}\nfrom numba import jit\nimport numpy as np\n```\n:::\n\n\n## Test it\n\n- `@jit` is the much-ballyhoo'ed *decorator*.\n\n::: {#6f92e909 .cell execution_count=3}\n``` {.python .cell-code}\nx = np.arange(100).reshape(10, 10)\n\n@jit\ndef go_fast(a): # Function is compiled to machine code when called the first time\n    trace = 0.0\n    for i in range(a.shape[0]):   # Numba likes loops\n        trace += np.tanh(a[i, i]) # Numba likes NumPy functions\n    return a + trace              # Numba likes NumPy broadcasting\n\n_ = go_fast(x)\n```\n:::\n\n\n## Aside: **pandas**\n\n- Numba does *not* accelerate **pandas**\n- It is the \"numerical\" not \"data\" compiler.\n- Don't do this:\n\n::: {#93cea28f .cell execution_count=4}\n``` {.python .cell-code}\nimport pandas as pd\n\nx = {'a': [1, 2, 3], 'b': [20, 30, 40]}\n\n@jit(forceobj=True, looplift=True) # Need to use object mode, try and compile loops!\ndef use_pandas(a): # Function will not benefit from Numba jit\n    df = pd.DataFrame.from_dict(a) # Numba doesn't know about pd.DataFrame\n    df += 1                        # Numba doesn't understand what this is\n    return df.cov()                # or this!\n\n_ = use_pandas(x)\n```\n:::\n\n\n## Modes\n\n- Numba basically has two modes\n  \t- `object mode`: Slow Python mode, works for **pandas**\n\t- `nonpython`: Fast compiled mode, works for NumPy\n- I wouldn't bother with Numba unless you can use `nonpython`\n  \t- Basically Numba doesn't do anything.\n\n# Timing\n\n## time\n\n- It is hard to motivate Numba without seeing how fast it is.\n- We will introduce `time`\n\n::: {#5ea5409f .cell execution_count=5}\n``` {.python .cell-code}\nimport time\n\ntime.time()\n```\n\n::: {.cell-output .cell-output-display execution_count=79}\n```\n1749008449.5707188\n```\n:::\n:::\n\n\n## perf_counter()\n\n- `time` supports `time.perf_counter()`\n\n> Return the value (in fractional seconds) of a performance counter, i.e. a clock with the highest available resolution to measure a short duration.\n\n- Let's try it out.\n\n## NumPy\n\n- I have have claimed NumPy vector operations are faster than base Python.\n- Let's test it.\n\n::: {#391fc49c .cell execution_count=6}\n``` {.python .cell-code}\npy_lst = list(range(1000000))\nnp_arr =  np.arange(1000000)\n\nlen(py_lst), len(np_arr)\n```\n\n::: {.cell-output .cell-output-display execution_count=80}\n```\n(1000000, 1000000)\n```\n:::\n:::\n\n\n## Polynomial\n\n- We will write a simple polynomial.\n\n> [The Legendre polynomials were first introduced in 1782 by Adrien-Marie Legendre[5] as the coefficients in the expansion of the Newtonian potential...](https://en.wikipedia.org/wiki/Legendre_polynomials)\n\n> [...served as the fundamental gravitational potential in Newton's law of universal gravitation.](https://en.wikipedia.org/wiki/Newtonian_potential)\n\n## $P_{10}(x)$\n\n- We take $P_{10}(x)$, the 10th Legendre polynomial.\n\n$$\n\\tfrac{146189x^{10}-109395x^8+90090x^6-30030x^4+3465x^2-63}{256}\n$$\n\n- This uses only NumPy vectorizable operations:\n\n::: {#72599a36 .cell execution_count=7}\n``` {.python .cell-code}\ndef p_ten(x):\n\treturn (46189*x**10-109395*x**8+90090*x**6-30030*x**4+3456*x*2-63)//256\n\np_ten(np.arange(5))\n```\n\n::: {.cell-output .cell-output-display execution_count=81}\n```\narray([       -1,        14,     96060,   8097412, 162596541])\n```\n:::\n:::\n\n\n## Timing\n\n- It is a straightforward matter to time Python vs NumPy\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {#c4ad5541 .cell execution_count=8}\n``` {.python .cell-code}\nt = time.perf_counter()\n[p_ten(i) for i in py_lst] # Pythonic vector\npy_t = time.perf_counter() - t\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {#d1f47f86 .cell execution_count=9}\n``` {.python .cell-code}\nt = time.perf_counter()\np_ten(np_arr) # NumPy vectorization\nnp_t = time.perf_counter() - t\n```\n:::\n\n\n:::\n\n::::\n\n- Compare the times - NumPy 24x faster.\n\n::: {#1b1aed77 .cell execution_count=10}\n``` {.python .cell-code}\npy_t, np_t, py_t/np_t, np_t/py_t\n```\n\n::: {.cell-output .cell-output-display execution_count=84}\n```\n(0.9679413000121713,\n 0.04020869988016784,\n 24.07293204945404,\n 0.04154043213122763)\n```\n:::\n:::\n\n\n# Compilation\n\n## Compiling\n\n- To compile a Numba function, we have to run at least once.\n- We will:\n  \t- Write a function.\n\t- Time it with NumPy\n\t- Add Numba decorators\n\t- Run once\n\t- Time it with Numba\n- Helpfully, we timed `p_ten()` with NumPy\n\n\n## Numba time.\n\n- Declare with `@jit`\n\n::: {#42efdfc0 .cell execution_count=11}\n``` {.python .cell-code}\n@jit\ndef p_ten_nb(x):\n\treturn (46189*x**10-109395*x**8+90090*x**6-30030*x**4+3456*x*2-63)//256\n```\n:::\n\n\n- Compile by running once...\n\n::: {#a49dc490 .cell execution_count=12}\n``` {.python .cell-code}\n_ = p_ten_nb(np_arr)\n```\n:::\n\n\n- Time on the same array\n\n::: {#7fae1d7b .cell execution_count=13}\n``` {.python .cell-code}\nt = time.perf_counter()\np_ten_nb(np_arr) # NumPy vectorization\nnb_t = time.perf_counter() - t\nnb_t\n```\n\n::: {.cell-output .cell-output-display execution_count=87}\n```\n0.003835099982097745\n```\n:::\n:::\n\n\n## Maybe 10x?\n\n- At first, not great!\n- Let's optimize.\n\n::: {#9db18dd8 .cell execution_count=14}\n``` {.python .cell-code}\nnb_t, np_t, nb_t/np_t, np_t/nb_t\n```\n\n::: {.cell-output .cell-output-display execution_count=88}\n```\n(0.003835099982097745,\n 0.04020869988016784,\n 0.09537985544241219,\n 10.484394166478616)\n```\n:::\n:::\n\n\n## Using `jit`\n\n> [Numba provides several utilities for code generation, but its central feature is the numba.jit() decorator. Using this decorator, you can mark a function for optimization by Numbaâ€™s JIT compiler. Various invocation modes trigger differing compilation options and behaviours.](https://numba.pydata.org/numba-doc/dev/user/jit.html)\n\n## Parallel\n\n- Declare\n\n::: {#96dd2241 .cell execution_count=15}\n``` {.python .cell-code}\n@jit(nopython=True, parallel=True)\ndef p_ten_par(x):\n\treturn (46189*x**10-109395*x**8+90090*x**6-30030*x**4+3456*x*2-63)//256\n\n_ = p_ten_par(np_arr)  \n```\n:::\n\n\n- Time\n\n::: {#9b74c3d4 .cell execution_count=16}\n``` {.python .cell-code}\nt = time.perf_counter()\np_ten_par(np_arr) # NumPy vectorization\npt_t = time.perf_counter() - t\npt_t\n```\n\n::: {.cell-output .cell-output-display execution_count=90}\n```\n0.0017589000053703785\n```\n:::\n:::\n\n\n## For me\n\n- My device is about twice as fast when parallelized.\n- I suspect much faster if I wasn't developing these slides while running servers, etc. in the background.\n\n::: {#4ba88250 .cell execution_count=17}\n``` {.python .cell-code}\n# pt = parallel=true time\npt_t, nb_t, pt_t/nb_t, nb_t/pt_t\n```\n\n::: {.cell-output .cell-output-display execution_count=91}\n```\n(0.0017589000053703785,\n 0.003835099982097745,\n 0.45863211222156597,\n 2.1803968220980092)\n```\n:::\n:::\n\n\n## Or...\n\n- Or maybe I just need more numbers.\n\n::: {#a33fdfe0 .cell execution_count=18}\n``` {.python .cell-code}\nnp_arr = np.arange(100000000, dtype=np.int64) # 100 mil\n```\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {#c5027f31 .cell execution_count=19}\n``` {.python .cell-code}\n@jit(nopython=True, parallel=False)\ndef p_ten(x):\n\treturn (46189*x**10-109395*x**8+90090*x**6-30030*x**4+3456*x*2-63)//256\n\np_ten(np_arr) # Compile\nt = time.perf_counter()\np_ten(np_arr)\npf = time.perf_counter() - t\npf\n```\n\n::: {.cell-output .cell-output-display execution_count=93}\n```\n0.36344839981757104\n```\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {#1a91dedd .cell execution_count=20}\n``` {.python .cell-code}\n@jit(nopython=True, parallel=True)\ndef p_ten(x):\n\treturn (46189*x**10-109395*x**8+90090*x**6-30030*x**4+3456*x*2-63)//256\n\np_ten(np_arr) # Compile\nt = time.perf_counter()\np_ten(np_arr)\npt = time.perf_counter() - t\npt\n```\n\n::: {.cell-output .cell-output-display execution_count=94}\n```\n0.1386922001838684\n```\n:::\n:::\n\n\n:::\n\n::::\n\n## GIL\n\n- Python has a \"Global Interpreter Lock\" to ensure consistency of array operations.\n- All our array operations are independent, so we don't have to worry about any of that, I think.\n- We use `nogil`\n\n## nogil=True\n\n::: {#e311f480 .cell execution_count=21}\n``` {.python .cell-code}\n@jit(nopython=True, nogil=True, parallel=True)\ndef p_ten(x):\n\treturn (46189*x**10-109395*x**8+90090*x**6-30030*x**4+3456*x*2-63)//256\n\np_ten(np_arr) # Compile\nt = time.perf_counter()\np_ten(np_arr)\nng = time.perf_counter() - t\nng, pt # no gil, parallel true\n```\n\n::: {.cell-output .cell-output-display execution_count=95}\n```\n(0.14191369991749525, 0.1386922001838684)\n```\n:::\n:::\n\n\n- Doesn't do much here.\n\n## Signatures\n\n- Numba probably infers this, but it also benefits from knowing what kind of integer we are working with.\n- These are like the NumPy types.\n- Probably the biggest value... \n\n::: {#9c4a1532 .cell execution_count=22}\n``` {.python .cell-code}\nbiggest = p_ten(max(np_arr))\nbiggest, np.iinfo(np.int32), np.iinfo(np.int64)\n```\n\n::: {.cell-output .cell-output-display execution_count=96}\n```\n(22357606058205098,\n iinfo(min=-2147483648, max=2147483647, dtype=int32),\n iinfo(min=-9223372036854775808, max=9223372036854775807, dtype=int64))\n```\n:::\n:::\n\n\n- Fits in `np.int64`\n\n## cfunc\n\n- If we know the type of the values, we can use `cfunc` instead of `jit`\n- Compiles to C, may be faster!\n- Signatures are `out(in)`, so `/` is `float(int, int)`\n\n::: {#c9c0205e .cell execution_count=23}\n``` {.python .cell-code}\nx = 5\ny = 2\nz = x / y\ntype(z), type(x), type(y)\n```\n\n::: {.cell-output .cell-output-display execution_count=97}\n```\n(float, int, int)\n```\n:::\n:::\n\n\n## Run it\n\n::: {#386befd8 .cell execution_count=24}\n``` {.python .cell-code}\nfrom numba import cfunc, int64\n\n@cfunc(int64(int64))\ndef p_ten(x):\n\treturn (46189*x**10-109395*x**8+90090*x**6-30030*x**4+3456*x*2-63)//256\n\np_ten(np_arr) # Compile\nt = time.perf_counter()\np_ten(np_arr)\ntime.perf_counter() - t\n```\n\n::: {.cell-output .cell-output-display execution_count=98}\n```\n3.9497982999309897\n```\n:::\n:::\n\n\n- Meh. Works better with formulas than polynomials (exponentials and the like).\n\n## Aside: Intel\n\n- I tried a few Intel packages Numba recommended.\n- I got no noticeable changes from either, but mention them.\n\n## Intel SVML\n\n- These slides were compiled on an Intel device.\n- Numba recommends SVML for Intel devices.\n\n> Intel provides a short vector math library (SVML) that contains a large number of optimised transcendental functions available for use as compiler intrinsics\n\n```{.bash code-line-number=\"false\"}\npython3 -m pip install intel-cmplr-lib-rt\n```\n\n## Threading\n\n- For parallelism, Numba recommends `tbb` (Intel) or OpenMP (otherwise).\n- They are not available on all devices.\n- I could use `tbb`\n```{.bash code-line-number=\"false\"}\npython3 -m pip install tbb\n```\n\n\n## Floats\n\n- Numba works just fine with floats!\n\n::: {#f26f1d7c .cell execution_count=25}\n``` {.python .cell-code}\nfloat_arr = np_arr / 7\n# add fastmath to decorator, change // to /\n# `njit` means `jit(nopython=True`\nfrom numba import njit\n\n@njit(parallel=True)\ndef p_ten(x):\n\treturn (46189.*x**10.-109395.*x**8.+90090.*x**6.-30030.*x**4.+3456.*x*2.-63.)/256.\n\np_ten(np_arr) # Compile\nt = time.perf_counter()\np_ten(np_arr)\nfloat_t = time.perf_counter() - t\nfloat_t\n```\n\n::: {.cell-output .cell-output-display execution_count=99}\n```\n0.8945227998774499\n```\n:::\n:::\n\n\n## Floats\n\n- I used integers, but if we use floats we should use the `fastmath` option.\n- It allows greater inaccuracy (remember float rounding) in exchange for faster operations.\n\n::: {#2ec759da .cell execution_count=26}\n``` {.python .cell-code}\n@njit(fastmath=True, parallel=True)\ndef p_ten(x):\n\treturn (46189.*x**10.-109395.*x**8.+90090.*x**6.-30030.*x**4.+3456.*x*2.-63.)/256.\n\np_ten(np_arr) # Compile\nt = time.perf_counter()\np_ten(np_arr)\nfast_t = time.perf_counter() - t\nfast_t, float_t, fast_t/float_t, float_t/fast_t\n```\n\n::: {.cell-output .cell-output-display execution_count=100}\n```\n(0.13804470002651215,\n 0.8945227998774499,\n 0.15432217048623506,\n 6.479950332795482)\n```\n:::\n:::\n\n\n# Mandelbrot\n\n## \n\n",
    "supporting": [
      "0A_numba_files\\figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}